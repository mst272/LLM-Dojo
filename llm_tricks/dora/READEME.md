# DoRA: Weight-Decomposed Low-Rank Adaptation

æ­¤ä¸ºDoraå¾®è°ƒæ–¹æ³•çš„å®ç°(ç›®å‰**huggingfaceä¹Ÿå·²é›†æˆdora**ï¼Œæ•…ä½¿ç”¨å¯ä»¥ç›´æ¥ä½¿ç”¨huggingfaceå¦‚ä¸‹ï¼Œæœ¬æ¨¡å—å¯ä»¥ä½œä¸ºè¯¦ç»†çš„**ç†è®ºå­¦ä¹ **)âš½

huggingfaceä¸­ä½¿ç”¨å¦‚ä¸‹ï¼ŒåŸºäºloraçš„åŸºç¡€ä¸Šï¼Œå¢åŠ use_doraå‚æ•°å³å¯ã€‚æœ¬é¡¹ç›®çš„è®­ç»ƒæ¡†æ¶ä¹Ÿæ”¯æŒdoraè®­ç»ƒã€‚
```python
from peft import LoraConfig

# Initialize DoRA configuration
config = LoraConfig(
    use_dora=True, ...
)
```




Implementation of "DoRA: Weight-Decomposed Low-Rank Adaptation" (Liu et al, 2024) https://arxiv.org/pdf/2402.09353.pdf


## ğŸ˜¸æŠ€æœ¯åšå®¢é“¾æ¥

- [çŸ¥ä¹:DoraåŸç†åŠä»£ç è®²è§£](https://zhuanlan.zhihu.com/p/695269522)

## Tipsï¼š
Doraæ˜¯åŸºäºLoraçš„å˜ä½“ï¼Œæ•…ä¹Ÿå¯¹Loraè¿›è¡Œäº†ç®€å•çš„ç¤ºä¾‹ã€‚


DoRAå¯ä»¥åˆ†ä¸¤æ­¥æè¿°ï¼Œå…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯å°†é¢„è®­ç»ƒçš„æƒé‡çŸ©é˜µåˆ†è§£ä¸ºå¹…åº¦å‘é‡ï¼ˆmï¼‰å’Œæ–¹å‘çŸ©é˜µï¼ˆVï¼‰ã€‚ç¬¬äºŒæ­¥æ˜¯å°†LoRAåº”ç”¨äºæ–¹å‘çŸ©é˜µVå¹¶å•ç‹¬è®­ç»ƒå¹…åº¦å‘é‡mã€‚

## å¦‚ä½•ä½¿ç”¨


dora_example.py ä¸­æœ‰è¯¦ç»†å®Œæ•´çš„ LoRAåŠDoRAè®­ç»ƒä¸éªŒè¯ï¼Œå»ºç«‹äº†ä¸€ä¸ªå°çš„æ¨¡å‹ä»è®­ç»ƒåˆ°éªŒè¯ç­‰å…¨éƒ¨è¿‡ç¨‹ã€‚

lora_and_dora.ipynb ç”¨äºè‡ªå·±è°ƒè¯•åŠå­¦ä¹ ï¼Œå¯ä»¥åœ¨å…¶ä¸­é€æ­¥è¿è¡Œä»¥ç†è§£å…¶åŸç†ã€‚

è¿è¡Œä»¥ä¸‹ä»£ç å¯å¾—åˆ°å®éªŒç»“æœ
```shell
python dora_example.py
```

## å®éªŒç»“æœå¦‚ä¸‹ï¼š
è¿è¡Œ dora_example.pyã€‚è¶…å‚æ•°è®¾ç½®å‚è€ƒæ–‡ä»¶å†…ã€‚å°æ¨¡å‹å…·æœ‰å±€é™æ€§ï¼Œå…·ä½“doraå’Œloraçš„å®é™…æ•ˆæœå¯¹æ¯”è¿˜éœ€è¦æ›´å¤šçš„å®éªŒã€‚

```python
Epoch: 001/001 | Batch 000/938 | Loss: 2.3010
Epoch: 001/001 | Batch 400/938 | Loss: 0.4533
Epoch: 001/001 | Batch 800/938 | Loss: 0.0464
Epoch: 001/001 training accuracy: 95.31%
Time elapsed: 0.11 min
Total Training Time: 0.11 min
Test accuracy: 96.88%
Epoch: 001/002 | Batch 000/938 | Loss: 0.1734
Epoch: 001/002 | Batch 400/938 | Loss: 0.0447
Epoch: 001/002 | Batch 800/938 | Loss: 0.1270
Epoch: 001/002 training accuracy: 96.88%
Time elapsed: 0.11 min
Epoch: 002/002 | Batch 000/938 | Loss: 0.0626
Epoch: 002/002 | Batch 400/938 | Loss: 0.2149
Epoch: 002/002 | Batch 800/938 | Loss: 0.1430
Epoch: 002/002 training accuracy: 95.31%
Time elapsed: 0.23 min
Total Training Time: 0.23 min
Test accuracy LoRA finetune: 96.88%
Epoch: 001/002 | Batch 000/938 | Loss: 0.1588
Epoch: 001/002 | Batch 400/938 | Loss: 0.1235
Epoch: 001/002 | Batch 800/938 | Loss: 0.0506
Epoch: 001/002 training accuracy: 100.00%
Time elapsed: 0.11 min
Epoch: 002/002 | Batch 000/938 | Loss: 0.1374
Epoch: 002/002 | Batch 400/938 | Loss: 0.0892
Epoch: 002/002 | Batch 800/938 | Loss: 0.0606
Epoch: 002/002 training accuracy: 95.31%
Time elapsed: 0.23 min
Total Training Time: 0.23 min
Test accuracy DoRA finetune: 98.44%
```
