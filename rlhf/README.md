# RLHF å¼ºåŒ–å­¦ä¹ æ¡†æ¶

æœ¬æ¡†æ¶ä½¿ç”¨ç®€æ´çš„ä»£ç åŸºäºHuggingfaceå¯¹å„ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†é›†æˆï¼Œä¾¿äºè‡ªå·±ä¿®æ”¹ä¸ä½¿ç”¨ï¼Œæ˜¯ä¸€ä¸ªè½»é‡åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚

ä¸»è¦èµ„æºæ˜¯åœ¨1-8å¼ 40G A100ä¸Šè¿›è¡Œå®éªŒï¼Œæ”¯æŒlora qlora åŠdeepspeedå•å¡æˆ–å¤šå¡è®­ç»ƒã€‚

ä¸»è¦åŒ…æ‹¬ä¸‰ç±»ï¼š

**1ã€RLHF**

**2ã€Knowledge Distillation (çŸ¥è¯†è’¸é¦)**

**3ã€Rejected Sampling (æ‹’ç»é‡‡æ ·) ï¼šå¾…æ›´æ–°**

## ç›®å½•

- [RLHF](#rlhf)
  - [ç›®å‰æ”¯æŒçš„RLHF](#ç›®å‰æ”¯æŒçš„rlhf)
  - [Quick Star](#quick-star)
    - [æ•°æ®æ ¼å¼è¦æ±‚](#æ•°æ®æ ¼å¼è¦æ±‚)
    - [æ•°æ®æ ¼å¼é€‰æ‹©](#æ•°æ®æ ¼å¼é€‰æ‹©)
    - [å¯åŠ¨è®­ç»ƒ](#å¯åŠ¨è®­ç»ƒ)
    - [æ³¨æ„äº‹é¡¹](#æ³¨æ„äº‹é¡¹)
  - [æ˜¾å­˜å®éªŒ](#æ˜¾å­˜å®éªŒ)
- [Knowledge Distillation](#knowledge-distillation)
  - [Quick Star](#quick-star-1)
- [æ„Ÿè°¢](#æ„Ÿè°¢)

## RLHF
### ç›®å‰æ”¯æŒçš„RLHF
å®è·µæ¥çœ‹ä¸»è¦çš„è®­ç»ƒæ–¹å¼å³ä¸ºå•è½®ã€‚

- âœ… Rewardæ¨¡å‹çš„è®­ç»ƒ
- âœ… RLOO
- âœ… PPO(æš‚æ—¶ä¸å¯ç”¨)
- âœ… SimPO
- âœ… CPO
- âœ… CPO-SimPO
- âœ… DPO
- âœ… KTO

### ğŸš€Quick Star

è‹¥æœ‰é—®é¢˜è¯·å°è¯• deepspeed==0.15.4/python==3.10, æˆ–è€…å‡ºç°lossã€rewards/chosenä¸ºnanæ—¶ï¼Œè¯·æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹çš„requirements.txtï¼ŒæŒ‰ç…§æ­¤ç‰ˆæœ¬å®‰è£…çœ‹æ˜¯å¦èƒ½è§£å†³ã€‚

ä¸€äº›æ½œåœ¨çš„é—®é¢˜ï¼Œæš‚æ—¶è¿˜æ²¡å¾—åˆ°è§£å†³æˆ–è€…æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼š

https://github.com/huggingface/alignment-handbook/issues/57

https://github.com/microsoft/DeepSpeed/issues/6793#issuecomment-2502620884

https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/issues/29

#### æ•°æ®æ ¼å¼è¦æ±‚
âœ… DPOã€CPOã€SimPOã€CPO-SimPO:

éœ€è¦æœ‰å¦‚ä¸‹å­—æ®µï¼š
- prompt
- chosen
- rejected

```json lines
{"prompt":[{"role":"user","content":"How are you?"}],"chosen":[{"role":"assistant","content":"fine"}],"rejected":[{"role":"assistant","content":"no"}]}
```
âœ… KTO:
- prompt
- completion
- label

æ¯”è¾ƒç‰¹æ®Š,ç›¸å½“äºchosençš„labelä¸ºtrue,rejectedçš„labelä¸ºfalseï¼š
```json lines
{"prompt":[{"role":"user","content":"How are you?"}],"completion":[{"role":"assistant","content":"fine"}],"label":true}
```

âœ… Reward:
- chosen
- rejected

```json lines
{"chosen":[{"role":"user","content":"How are you?"},{"role":"assistant","content":"fine"}],"rejected":[{"role":"user","content":"How are you?"},{"role":"assistant","content":"no"}]}
```
âœ… DPOã€RLOO:
- prompt

```json lines
{"prompt":[{"role":"user","content":"How are you?"}]}
```

#### æ•°æ®æ ¼å¼é€‰æ‹©

**1.è‡ªåŠ¨é€‚é…Chat Templateæ ¼å¼**: è¾“å…¥æ•°æ®éœ€ä¸ºuser assistantæ ‡å‡†æ¨¡å¼,å…·ä½“å¯è§ä¸Šè¿°æ•°æ®æ ¼å¼è¦æ±‚ã€‚

**2.ä¸ä½¿ç”¨Chatæ ¼å¼**: è¾“å…¥æ•°æ®ç›´æ¥æ”¹ä¸ºç›¸åº”å­—æ®µæ ¼å¼å³å¯ï¼Œä¾‹å¦‚:
```json lines
{"prompt":"How are you?","chosen":"fine", "rejected": "no"}
```

```json lines
{"chosen":"How are you? fine", "rejected": "How are you? no"}
```
è®­ç»ƒæ—¶ä¾¿ä¸ä¼šè¿›è¡Œé€‚é…ï¼Œé‡‡ç”¨åŸå§‹è¾“å…¥è¿›è¡Œè®­ç»ƒã€‚


#### å¯åŠ¨è®­ç»ƒ

ä¸¤ä¸ªå‚æ•°é…ç½®æ–‡ä»¶ï¼Œç¬¬ä¸€ä¸ªä¸º```common_args.py```, å…¶ä½™ä¸åŒæ–¹æ³•çš„é…ç½®åœ¨```rlhf_args```æ–‡ä»¶å¤¹å†…

å»ºè®®ä½¿ç”¨deepspeedå¯åŠ¨ï¼Œå¯åŠ¨è„šæœ¬åœ¨```rlhf_run.sh```
```bash
bash rlhf_run.sh
```

 - rlhf_type: [PPO,RLOO,CPO,DPO,SimPO,CPOSimPO,Reward]
 - train_mode: [lora, qlora, full]

#### æ³¨æ„äº‹é¡¹
1ã€éœ€è¦è‡ªå·±å»çœ‹AutoModelForSequenceClassificationæ˜¯å¦å¯ä»¥åŠ è½½å…¶Classificationæ¨¡å‹ï¼Œä¸èƒ½çš„è¯éœ€è¦åœ¨å…¶configæ–‡ä»¶ä¸­æ˜ å°„ã€‚

2ã€æ¶‰åŠåˆ°rewardæ¨¡å‹æ—¶ï¼Œéœ€è¦ä¸¤ä¸ªæ¨¡å‹çš„tokenizerç›¸åŒã€‚

3ã€ä½¿ç”¨deepspeedæ—¶éœ€è¦é€šè¿‡accelerateè¿›è¡Œä½¿ç”¨ï¼Œç›´æ¥deepspeedçš„è¯ä¼šæŠ¥é”™(ç›®å‰ä¼¼ä¹æ²¡æœ‰å¾ˆå¥½çš„è§£å†³æ–¹æ¡ˆ)

4ã€ä¸€èˆ¬æ¥è¯´trlçš„traineræ˜¯ä¸æ”¯æŒä½¿ç”¨deepspeedçš„optimizerå’Œschedulerçš„

5ã€ä¸æ”¯æŒQloraå’Œdeepspeed zero-3ï¼Œæ”¯æŒQloraå’Œdeepspeed zero-2

6ã€è®­ç»ƒQwen2æ—¶é‡åˆ°æŠ¥é”™ï¼Œæç¤º```no padding token is defined```ã€‚éœ€è¦åœ¨qwen2 ```config.json```ä¸­æ·»åŠ pad_token_id,åœ¨tokenizerä¸­è®¾ç½®æ²¡ç”¨ã€‚

7ã€PPO/RLOOå‚æ•°è§£é‡Šï¼š

See:https://github.com/huggingface/trl/issues/1740

The ``num_train_epochs`` and ``num_ppo_epochs`` are actually two different things. The num_train_epochs means how many epochs do we go over the dataset, the num_ppo_epochs means the number of epochs we perform PPO updates on a batch of data. So, there is a subtle but meaningful difference here.

8ã€CPOç³»åˆ—ä¸æ”¯æŒfp16ï¼Œæ”¯æŒbf16

#### æ˜¾å­˜å®éªŒ
res_lengthä¸º64

| **RLHF** | **deepspeed** | **æ–¹å¼** | **Reward Model** | **SFT Model**  | **æ˜¾å­˜å ç”¨**               |
|----------|---------------|--------|------------------|----------------|------------------------|
| RLOO     | Zero 3        | Lora   | QWEN2(7B)        | QWEN2(7B)      | 2 x A100(40GB): 15~30G |
| RLOO     | Zero 3        | Full   | QWEN2(7B)        | QWEN2(7B)      | 2 x A100(40GB): é€Ÿåº¦å¾ˆæ…¢   |
| RLOO     | Zero 2        | Qlora  | QWEN2(7B)        | QWEN2(7B)      | 2 x A100(40GB): 30~40G |
| PPO      | Zero 2        | Lora   | MiniCPM(2B)      | Deepseek(6.7B) | 2 x A100(40GB): OOM    |
| PPO      | Zero 3        | Lora   | MiniCPM(2B)      | Deepseek(6.7B) | 2 x A100(40GB): 20-25G |
| PPO      | Zero 2        | Qlora  | MiniCPM(2B)      | Deepseek(6.7B) | 2 x A100(40GB): 30G    |

## Knowledge Distillation
ç›®å‰æ”¯æŒä¸‰ç§ç±»å‹çš„çŸ¥è¯†è’¸é¦ï¼ŒGKDæ•ˆæœæœ€å¥½ï¼š
- Supervised KD(off-policy)
- SeqKD(off-policy)
- GKD(on-policy)

å…·ä½“ä»‹ç»å¯å‚è§æ–‡ç« ï¼š[çŸ¥è¯†è’¸é¦](https://zhuanlan.zhihu.com/p/1064724364)

### Quick Star
è¿›å…¥scriptç›®å½•ä¸‹bashè¿è¡Œ```gkd_run.sh```å³å¯ï¼Œä¿®æ”¹å¯¹åº”å‚æ•°è¿è¡Œã€‚åŒæ ·æ”¯æŒDeepspeed.


```bash
bash gkd_run.sh
```

**å‚æ•°ä»‹ç»**ï¼š
- lmbdaï¼š0æ—¶ä¸ºSupervised KDï¼Œ1æ—¶ä¸ºGKDã€‚å¯åœ¨[0,1]èŒƒå›´å†…é€‰æ‹©ï¼Œè¿™æ ·å°±ä¼šæ··åˆæ¯”ä¾‹
- beta:  0æ—¶lossä¸ºKLDï¼Œ 1æ—¶ä¸ºJSDã€‚å¯åœ¨[0,1]èŒƒå›´å†…é€‰æ‹©ï¼Œè¿™æ ·å°±ä¼šæ··åˆæ¯”ä¾‹
- seq_kd: Trueæ—¶Supervised KDå°†æ›¿æ¢ä¸ºSeq KDï¼Œé»˜è®¤ä¸ºFalseï¼Œå…¶ä»–ä¸å˜ã€‚
- model_name_or_pathï¼šStudent Modelï¼Œå³ä½ éœ€è¦è®­ç»ƒçš„æ¨¡å‹
- teacher_model_name_or_pathï¼šTeacher Model, ä¸è®­ç»ƒã€‚

## Rejected Sampling
å¾…æ›´æ–°

## æ„Ÿè°¢

ç‰¹åˆ«æ„Ÿè°¢huggingface trlåšå‡ºçš„å¼ºå¤§è´¡çŒ®ï¼Œé€šè¿‡ trl æˆ‘ä»¬çœŸçš„å¯ä»¥å¾ˆå®¹æ˜“ç®€æ´çš„å®ç°RLHFã€‚